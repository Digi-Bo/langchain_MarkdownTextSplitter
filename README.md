
# üìò Application de questions/r√©ponses portant sur un document avec LangChain : strat√©gie de chunking sur du markdown

Ce programme permet de poser des questions sur un document pr√©alablement charg√© et d'obtenir des r√©ponses pertinentes en utilisant un mod√®le d'intelligence artificielle. Le syst√®me exploite une base vectorielle pour rechercher le contexte appropri√© dans le document, puis g√©n√®re une r√©ponse via un mod√®le de chat. L'application propose deux interfaces :
- Une interface en ligne de commande (CLI) via `main.py`.
- Une interface web interactive via Streamlit avec `app.py`.


## Strat√©gie de chunking efficace appliqu√©e √† du Markdown avec **`MarkdownTextSplitter`**

Un article complet est d√©di√©  √† [La strat√©gie de chunking avec MarkdownTextSplitter ](./01-Doc/01-documentation.md) 




## Logique m√©tier dans ```utils.py```


``` mermaid:


graph TD
    A[init_resources] -->|Appelle| B[initialize_openai_key]
    A -->|Charge et d√©coupe| C[load_and_split_document]
    A -->|Cr√©e la base vectorielle| D[create_vector_store]
    A -->|Configure le mod√®le et les prompts| E[setup_templates_and_model]
    C -->|Fournit les fragments| D
    D -->|R√©cup√®re les documents pertinents| F[generate_response]
    F -->|Retourne la r√©ponse| G[Application]




```





---

## üåê Cr√©er et activer un environnement virtuel :

### **MacOS/Linux** :
```bash
python3 -m venv env
source env/bin/activate
```

### **Windows** :
```bash
python -m venv env
source env/bin/activate
```

---

## üèóÔ∏è Installation des d√©pendances :

### Installer Python 3.6 ou une version sup√©rieure

Utilisez `pip3` sur Mac ou Linux, et `pip` sous Windows :
```bash
pip install -r requirements.txt
pip install --upgrade langchain
```

---

## üîë Configuration des variables d'environnement :

Pour configurer correctement votre environnement, cr√©ez un fichier `.env` √† la racine du projet et ajoutez les informations suivantes :

```env
# Cl√© API OpenAI pour acc√©der aux mod√®les d'OpenAI
OPENAI_API_KEY=sk-cl√© open ai

# Activer le suivi avanc√© (tracing) pour LangChain
LANGCHAIN_TRACING_V2=true

# URL de l'endpoint LangSmith pour le suivi des performances
LANGCHAIN_ENDPOINT="https://eu.api.smith.langchain.com"

# Cl√© API LangSmith pour le suivi
LANGSMITH_API_KEY=cl√© langsmith

# Nom du projet de suivi (facultatif, pour organiser vos traces)
LANGCHAIN_PROJECT="Test_project"
```



### V√©rification :
Pour confirmer que les variables d'environnement sont bien d√©finies, utilisez cette commande :
```bash
echo $OPENAI_API_KEY  # Sous MacOS/Linux
echo %OPENAI_API_KEY%  # Sous Windows
```


---

## ‚ñ∂Ô∏è Lancer l'application en CLI :

Pour d√©marrer l'interface en ligne de commande :
```bash
python main.py
```

---

## ‚ñ∂Ô∏è Lancer l'application Streamlit (localhost:8501) :

Pour d√©marrer l'interface web interactive :
```bash
streamlit run app.py
```

---

## üìÑ Fonctionnement g√©n√©ral :
1. Le document source (`presentation-VPS.md`) est charg√© et segment√© en fragments pour permettre une recherche efficace.
2. Une base vectorielle est construite √† partir de ces fragments pour rechercher les parties pertinentes du document.
3. L'utilisateur pose une question via l'interface (CLI ou Streamlit).
4. Les fragments pertinents sont r√©cup√©r√©s et utilis√©s comme contexte pour g√©n√©rer une r√©ponse √† l'aide du mod√®le de chat.
5. La r√©ponse est affich√©e √† l'utilisateur.
